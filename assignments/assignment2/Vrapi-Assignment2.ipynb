{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb1a955",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d14fc5",
   "metadata": {},
   "source": [
    "##### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b44f90",
   "metadata": {},
   "source": [
    "1. Perceptron\n",
    "    - This is a simple binary classifier that can linearly separate two classes at a time (though potentially more with the OvA strategy).\n",
    "    - It operates by recieving a set of inputs and weights, running it through a net input function, passing that result to a threshold function which produces a binary output which is finally used to calculate the error and then update the weights.\n",
    "    - This does solve an optimization problem which is to minimize the error.\n",
    "    - It is very fast, not very strong or robust, and as far as I'm aware does not use either entropy or distance. It instead simply uses the binary decision rule to update its weights.\n",
    "    - Used in NLP, Computer Vision, Robotics and control systems, pattern recognition and more\n",
    "    \n",
    "2. SVM\n",
    "    - The support vector machine is a type of supervised ML algorithm that is used for both classification and regression. They are very good at handling high dimensional data, and unlike the perceptron, are able to handle classes which are not linearly separable by using the \"kernel trick\" to transform data into a higher dimensional space where they become linearly separable.\n",
    "    - Their main purpose is to find an optimal hyperplane that separates the classes in a dataset. They do this by maximizing the **distance** between the hyperplane and the nearest points to it (the support vectors) in an effort to generalize better and avoid overfitting. \n",
    "    - It is fast, strong and robust, and statistical, and are widely used all over the place for all kinds of tasks.\n",
    "    - It does solve an optimization problem (in order to find the best hyperplane) which tries to find the weights and bias that minimize the cost function while maximizing the margin. This is also typically done with quadratic programming or convex optimization like gradient descent.\n",
    "    - Used in image recognition, anomoly detection, time series analysis, text classification\n",
    "    \n",
    "3. Decision Tree\n",
    "    - The decision tree is a type of supervised learning algorithm that also can be used for both classification and regression tasks.\n",
    "    - It works by starting with a root node which encompasses the entire dataset, and then selects the best feature to split the data into subsets based on certain criteria like the information gain or gini impurity. It aims to maximize the separation of classes or to minimize the impurity. Once the data is split into subsets based on the selected feature, the process is repeated recursively for each of the subsets up to a maximum depth/minimum number of samples in leaf nodes/or when further splitting does not improve the models performance.\n",
    "    - These are very good models which are able to capture non-linear relationships between features, however they are very prone to overfitting (where they would create trees that are overly complex, possibly way too deep, and therefore do not generalize to unseen data).\n",
    "    - They do solve an optimization problem -- the criteria being the Gini impurity, information gain, or entropy. MSE can be used as an entroy metric for the cost function.\n",
    "    - These are used in fraud detection, anomoly detection, medical diagnoses\n",
    "\n",
    "4. Random Forest\n",
    "    - This is an ensemble learning method that combines multiple decision trees which are trained on different subsets of the training data (independently). The features each tree is trained on are selected at random as well in order to introduce diversity to the trees which helps it prevent overfitting.\n",
    "    - Random forests are slow if there are a large number of trees underneath, but fast to train. Predictions can be very slow once they are trained as well.\n",
    "    - They are very robust, and can handle missing data without imputation.\n",
    "    - This, like the SVM, is also very broadly used.\n",
    "    - They solve many optimization problems (because they are a collection of decision trees) in the same way the decision tree does during training, but over the ensemble of trees. During prediction, however, they also solve an optimization problem which is typically done by majority voting for classification or averaging for regression tasks.\n",
    "    - Both random forests and decision trees are naturally suited to multi-class problems\n",
    "    - Used in recommendation systems, NLP, financial analysis, anomaly detection, image and object recognition and bioinformatics, among others\n",
    "    \n",
    "I think the model I would try first given a random dataset and problem I am trying to solve would most likely be the simplest and fastest to execute model in order to validate that my pipeline has been set up correctly. The perceptron here is simple, as is NaiveBayes, so either one of those I think I would try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ff047",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb576e18",
   "metadata": {},
   "source": [
    "1. Numerical\n",
    "    - This is a feature whose value is a real number. In the iris data set, this is any one of the features, such as petal-width, which would be something along the lines of 1.2 (centimeters)\n",
    "2. Nominal\n",
    "    - This is data, sometimes refered to as categorical data, that represents categories or groups. Examples could be things like male/female, red/blue/green, ford/tesla/GM, USA/UK etc. They typically have to be one-hot encoded by mapping these categorical values into an integer set in order to use them within an ML algorithm.\n",
    "3. Date\n",
    "    - This is a feature which represents a date or point in time. An example would be \"June 10th, 2023\" or \"2012-01-01 00\\:00\\:00 UTC\" or \"92347298231\" ms after 1970.\n",
    "4. Text\n",
    "    - A text feature in a dataset is a string of characters. An example would be \"hello\", or \"Hello, my name is Joni\". In the context of NLP specifically though, a text feature could be a specific characteristic that is extracted from textual data. This could be the frequency that that particular word shows up in some collection of text, it could be a vector embedding representing that text, an n-gram and much more.\n",
    "5. Image\n",
    "    - In order to use images in an ML algorithm, they have to be in a form that it can understand, and that representation is typically (as in the digits dataset) a 2d array of pixel values. This means that from (0, 0), the origin of the image, lets say the top left corner of it, each element in the array would correspond to the rgb value of the pixel in the image at that location.\n",
    "6. Dependent Variable\n",
    "    - The dependant variable, also referred to as the target variable, is the feature you are trying to predict. An easy way to understand this is via a classification task (such as the one we did last homework on the iris dataset). The iris dataset comes with a set of data points, and a corresponding target column which labels each one of those datapoints as being a particular type of flower such as \"virginica\" or \"versicolor\". If you were trying to classify cars by make, it would be \"ford\" or \"tesla\" etc. In a regression task, this may be something like age, the price of something, in analyzing war, this may be the number of units lost etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84725779",
   "metadata": {},
   "source": [
    "##### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b67e2",
   "metadata": {},
   "source": [
    "Before we talk about these performance metrics, lets first define a few basic terms:\n",
    "\n",
    "1. True Positive (TP)\n",
    "    - These are the data elements that are actually positive, and are correctly predicted to be positive.\n",
    "2. True Negative (TN)\n",
    "    - These are the data elements that are actually negative, and are correctly predicted to be negative.\n",
    "3. False Positive (FP)\n",
    "    - These are the data elements that are actually negative, but are predicted to be positive.\n",
    "4. False Negative (FN)\n",
    "    - These are the data elements that are actually positive, but are predicted to be negative.\n",
    "    \n",
    "Now, lets talk about some classifier performance metrics:\n",
    "\n",
    "1. Error (ERR)\n",
    "    - This is a metric which shows you how often your model gets a prediction wrong. It is calculated as:\n",
    "    \n",
    "    $ERR = \\frac{FP + FN}{FP + FN + TP + TN}$\n",
    "\n",
    "2. Accuracy (ACC)\n",
    "    - This is a metric that shows you how often your model gets a prediction right. It is calculated as:\n",
    "    \n",
    "    $ACC = \\frac{TP + TN}{FP + FN + TP + TN} = 1 - Error$\n",
    "    \n",
    "3. False Positive Rate (FPR)\n",
    "    - This is the ratio of false positives relative to the total number of negatives. It provides insight as to how often a false positive will be predicted. It is calculated as:\n",
    "    \n",
    "    $FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}$\n",
    "    \n",
    "4. True Positive Rate (TPR) (Also called Recall (REC) or Sensitivity)\n",
    "    - This is the rate of true positives relative to the total number of positives. It aims to tell you about the model's ability to detect positive instances of a class. It is calculated as:\n",
    "    \n",
    "    $TPR = \\frac{TP}{P} = \\frac{TP}{FN + TP}$\n",
    "    \n",
    "5. True Negative Rate (TNR) (Also called Specificity)\n",
    "    - This is the proportion of of true negatives out of the total number of negatives. This aims to tell you the same thing recall tells you about true positives, but about true negatives. This would be useful in a situation where false positives have a significant cost, maybe like a system which tries to identify nuclear attacks. It is calculated as:\n",
    "    \n",
    "    $TNR = \\frac{TN}{TN + FP}$\n",
    "    \n",
    "6. Precision (PRE)\n",
    "    - This is the proportion of correctly predicted postives relative to all the positives predicted by the model. It provides information about the accuracy and reliability of the positive predictions made by the model. It is calculated as:\n",
    "    \n",
    "    $PRE = \\frac{TP}{TP + FP}$\n",
    "    \n",
    "7. F1 Score\n",
    "    - This is the harmonic mean of precision and recall. It combines the model's ability to capture positive instances with its positive predictive value. This score is useful when you want to consider both precision and recall at the same time. It is calculated as:\n",
    "    \n",
    "    $F1 = 2\\frac{PRE \\cdot REC}{PRE + REC}$\n",
    "    \n",
    "8. Reciever Operating Characteristic (ROC)\n",
    "    - This is the ratio of the TPR to the FPR:\n",
    "    \n",
    "    $\\frac{TPR}{FPR}$\n",
    "    \n",
    "    - It is typically represented as a curve with TPR plotted on the y-axis and FPR plotted on the x-axis. A model that classifies things at random will show up as a diagonal line across this plot. A good classifer will hug the top left portion of the graph, indicating that it has a high true positive rate, and a low false positive rate. Likewise, a poor curve will be the inverse of this with a high false positive rate, and a low true positive rate.\n",
    "\n",
    "9. Area Under the ROC Curve (AUC-ROC)\n",
    "    - This is a quantitative measurement of the overall performance of the classifier using the ROC curve. It boils the ROC curve down to a single value. A higher value here will indicate a better classifier. This would be calculated as the definite integral of the roc curve, but can also be approximated with Simpsons rule.\n",
    "\n",
    "10. LogLoss (Cross-Entropy Loss)\n",
    "    - This is a metric that is used to evaluate the accuracy of a probablistic classifier by measuring its false classifications. It penalizes the classifier more heavily for incorrect predictions that are made with high confidence. In essence, it tries to tell you how accurate the model is overall, weighing the \"less impactful\" incorrect predictions less than the \"more impactful\" ones. Minimizing the LogLoss generally makes the classifier more accurate. It can be generalized to account for a multi-class classifier, however for a binary classifier, the log loss is:\n",
    "    \n",
    "    $\\frac{1}{n} \\cdot \\sum [y \\cdot log(p) + (1-y) \\cdot log(1-p)]$\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
