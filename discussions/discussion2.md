The famous science fiction author Isaac Asimov stated "Three Laws of Robotics" rules or laws in his novels. These laws, incorporated into almost all of the positronic robots appearing in his fiction, cannot be bypassed, being intended as a safety feature. The rules are:

A robot may not injure a human being or, through inaction, allow a human being to come to harm.
A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

Investigate these three rules and align them with the concepts/opinions mentioned in the Russell, Hauert, Altman, & Veloso (2015) article.

---

In my opinion, as it pertains to these three laws, the only researcher with relevant commentary from the article in Nature was Stuart Russell with his discussion on LAWS (lethal autonomous weapons systems). The other researchers mention robot-human cohabitation, distribution of the benefits of AI, as well as shaping the public debate around AI, none of which I believe to be relevant to these three rules as Asimov's rules are practical in nature and revolve around the physical implementation of AI enabled systems in our lives.

For Russell, the overriding concern about AI, or AI enabled systems, should be what he believes is the most likely endpoint of AI in military machines...machines that ultimately become most efficient at killing the adversary by not having a human at any point in the kill-chain. In Russell's view, the limiting factor in the capabilities of these LAWS will not be the performance of the AI systems that power their functionality, but will instead most likely be the fundamental physics behind the tasks they are trying to accomplish.

Asimov creates a world where robots are almost sentient -- not in the same way humans are, but in many of the ways that matter. They are able to learn, problem solve, and are capable of complex reasoning, but are not self-aware in the same way humans are. Their actions are also limited by the 3 laws in a fundamentally programmatic sense in a way that we are not limited by the laws of our governments. The laws, however, are clearly subjective. For example, what is meant by "harm" in the first rule? The obvious answer is something that physically hurts them...but is that always harmful? If a human being who is under the care of one of these robots is a drug addict and the robot takes away their drugs, then they go into withdrawal...is that considered harm? If one person is attacking another person and the only way to stop that person from getting hurt is to hurt the attacker...is that considered harm? The conflicts and subjective reasoning further increase from here as you move your way down the list.

I believe the answers to these questions are inherently subjective. I will think some actions are acceptable that you will not, and I believe that is the fundamental point that Russell is trying to make in advocating for a human in the kill-chain. Taking a life should require morality he says, and history has proven that the more you detach decision makers from the consequences of their actions, the more easily they are able to put away their morality in favor of violent action. The kings and queens several hundred years ago lived a life of opulence, completely disconnected from the real world. They, in turn, had no problem warring constantly at the expense of everyone else. Today, with the advent of drones, military leadership around the world are able to drop bombs wherever they need to be dropped without putting a pilot's life (or sanity) at risk...and so they bomb more often.

The natural extension of this, Russell believes, is that as it becomes easier and easier to kill effectively, more killing will happen, and more effective means of killing will be pursued -- the natural end state of which will be the perfect killing machine. One that doesn't think or feel, that doesn't get PTSD or have second thoughts about carrying out orders; one that simply, and with ruthless efficiency, kills its targets.

Asimov's rules do not, and would not, prevent this. They simply are not well enough defined. Human morality is not well enough defined, and until the day that it is, there should be a human who can explain himself pulling the trigger.
